{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from utils import Encoder, Decoder, additive_white_gaussian_noise_channel, ser_mqam_awgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_TRAIN = {\n",
    "    \"M\": 16,  # Number of constellation points\n",
    "    \"flag_train_model\": True,  # Flag to control training\n",
    "    \"training_snr\": 12,  # Training SNR (dB)\n",
    "    \"best_model_path\": \"./model/ae_siso_awgn_16qam_best_model.pth\",  # Path to save the best model\n",
    "    \"latest_checkpoint_path\": \"./model/ae_siso_awgn_16qam_latest_checkpoint.pth\",  # Path to save the latest checkpoint\n",
    "    \"best_encoder_path\": \"./model/ae_siso_awgn_16qam_best_encoder.pth\",  # Path to save the best encoder\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder([CONFIG_TRAIN[\"M\"], 10, 10, 2]).to(device)\n",
    "decoder = Decoder([2, 256, 256, CONFIG_TRAIN[\"M\"]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, loss, optimizer, model_type=\"latest\"):\n",
    "    \"\"\"\n",
    "    Save the model to a file.\n",
    "    - encoder: the encoder model\n",
    "    - decoder: the decoder model\n",
    "    - loss: the loss history\n",
    "    - optimizer: the optimizer\n",
    "    - model_type: \"latest\"(default) or \"best\"\n",
    "    \"\"\"\n",
    "\n",
    "    if model_type == \"best\":  # Decide the filename based on model_type\n",
    "        filename = CONFIG_TRAIN[\"best_model_path\"]\n",
    "    else:\n",
    "        filename = CONFIG_TRAIN[\"latest_checkpoint_path\"]\n",
    "    torch.save(\n",
    "        {\n",
    "            \"Encoder\": encoder.state_dict(),\n",
    "            \"Decoder\": decoder.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "def save_encoder(encoder):\n",
    "    \"\"\"\n",
    "    Save the encoder model to a file.\n",
    "    - encoder: the encoder model\n",
    "    \"\"\"\n",
    "\n",
    "    torch.save(encoder.state_dict(), CONFIG_TRAIN[\"best_encoder_path\"])\n",
    "\n",
    "\n",
    "def early_stopping(loss_hist, patience=5):\n",
    "    \"\"\"\n",
    "    Check if the training should be stopped early.\n",
    "    - loss_hist: list of loss values\n",
    "    - patience: number of epochs to wait before stopping\n",
    "    \"\"\"\n",
    "\n",
    "    if len(loss_hist) > patience:\n",
    "        if all(\n",
    "            x > loss_hist[-1] for x in loss_hist[-(patience + 1) :]\n",
    "        ):  # if the last patience losses are decreasing\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    encoder, decoder, optimizer, iterations, loss_hist, num_messages, device\n",
    "):\n",
    "    criterion = nn.NLLLoss()  # negative log likelihood loss\n",
    "    best_loss = float(\"inf\")  # Initialize the best loss to infinity\n",
    "    try:\n",
    "        for iterator in tqdm(\n",
    "            range(len(loss_hist), iterations), desc=\"training process\"\n",
    "        ):\n",
    "            messages = torch.randint(\n",
    "                0, CONFIG_TRAIN[\"M\"], size=(num_messages,), device=device\n",
    "            )\n",
    "            one_hot = F.one_hot(messages, CONFIG_TRAIN[\"M\"]).float()\n",
    "            tx = encoder(one_hot)\n",
    "            rx = additive_white_gaussian_noise_channel(tx, CONFIG_TRAIN[\"training_snr\"])\n",
    "            y_pred = decoder(rx)\n",
    "\n",
    "            loss = criterion(y_pred, messages)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                save_model(encoder, decoder, loss_hist, optimizer, model_type=\"best\")\n",
    "                save_encoder(encoder)\n",
    "\n",
    "            if (iterator % 200) == 0:\n",
    "                save_model(encoder, decoder, loss_hist, optimizer, model_type=\"latest\")\n",
    "                save_encoder(encoder)\n",
    "\n",
    "            if early_stopping(loss_hist):\n",
    "                save_model(encoder, decoder, loss_hist, optimizer, model_type=\"latest\")\n",
    "                save_encoder(encoder)\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        save_model(encoder, decoder, loss_hist, optimizer, model_type=\"latest\")\n",
    "        print(\"Training interrupted\")\n",
    "\n",
    "    save_model(encoder, decoder, loss_hist, optimizer, model_type=\"latest\")\n",
    "    save_encoder(encoder)\n",
    "    # Plot the loss\n",
    "    plt.semilogy(loss_hist)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG_TRAIN[\"flag_train_model\"]:\n",
    "    # check if there is a checkpoint to resume training\n",
    "    if os.path.exists(CONFIG_TRAIN[\"latest_checkpoint_path\"]):\n",
    "        checkpoint = torch.load(\n",
    "            CONFIG_TRAIN[\"latest_checkpoint_path\"], map_location=device\n",
    "        )\n",
    "        # load the model, optimizer a loss history\n",
    "        encoder.load_state_dict(checkpoint[\"Encoder\"])\n",
    "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
    "\n",
    "    parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = Adam(parameters, lr=0.01)\n",
    "\n",
    "    if os.path.exists(CONFIG_TRAIN[\"latest_checkpoint_path\"]):\n",
    "        # optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        loss_hist = checkpoint[\"loss\"]\n",
    "        print(f\"Resuming training from iterator {len(loss_hist)}\")\n",
    "    else:\n",
    "        loss_hist = []\n",
    "        print(\"Training from scratch\")\n",
    "\n",
    "    iterations = int(1e2)  # Total number of epochs to train\n",
    "    num_messages = int(1e4)  # Number of messages to use for training (batch size)\n",
    "    if iterations > len(loss_hist):\n",
    "        train_model(\n",
    "            encoder,\n",
    "            decoder,\n",
    "            optimizer,\n",
    "            iterations,\n",
    "            loss_hist,\n",
    "            num_messages,\n",
    "            device,\n",
    "        )\n",
    "    else:\n",
    "        print(\"epochs already completed\")\n",
    "else:\n",
    "    # check if there is a checkpoint to load the model\n",
    "    if os.path.exists(CONFIG_TRAIN[\"best_model_path\"]):\n",
    "        checkpoint = torch.load(CONFIG_TRAIN[\"best_model_path\"], map_location=device)\n",
    "        encoder.load_state_dict(checkpoint[\"Encoder\"])\n",
    "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
    "        print(\"Model loaded. Training iterator: \", len(checkpoint[\"loss\"]))\n",
    "    else:\n",
    "        print(\n",
    "            \"Model not found, please set flag_train_model to True and train the model\"\n",
    "        )\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(CONFIG_TRAIN[\"best_model_path\"], map_location=device)\n",
    "encoder.load_state_dict(checkpoint[\"Encoder\"])\n",
    "decoder.load_state_dict(checkpoint[\"Decoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_dB = np.arange(0, 22, 2)\n",
    "SER = np.array([])\n",
    "\n",
    "iteration = tqdm(range(len(SNR_dB)), desc=\"simulation process\")\n",
    "for iterator in iteration:\n",
    "    snr = SNR_dB[iterator]\n",
    "    num_mess = 6400  # number of messages to test\n",
    "    minErr = 1  # minimum number of errors\n",
    "    minSym = 1e6  # minimum number of symbols\n",
    "    totSym = 0  # total number of symbols\n",
    "    totErr = 0  # total number of errors\n",
    "    while totErr < minErr or totSym < minSym:\n",
    "        messages = torch.randint(0, CONFIG_TRAIN[\"M\"], size=(num_mess,)).to(device)\n",
    "        one_hot = F.one_hot(messages).float()\n",
    "        tx = encoder(one_hot)\n",
    "        rx = additive_white_gaussian_noise_channel(tx, snr)\n",
    "        rx_constant = (\n",
    "            rx.clone().detach().requires_grad_(False)\n",
    "        )  # no gradients in the channel model\n",
    "\n",
    "        y_pred = decoder(rx_constant)\n",
    "\n",
    "        m_hat = torch.argmax(y_pred, -1)\n",
    "\n",
    "        err = torch.sum(torch.not_equal(messages, m_hat)).to(\"cpu\").detach().numpy()\n",
    "\n",
    "        totErr += err\n",
    "        totSym += num_mess\n",
    "    SER = np.append(SER, totErr / totSym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SER_theory = ser_mqam_awgn(CONFIG_TRAIN[\"M\"], SNR_dB)\n",
    "# plot the SER-SNR curve\n",
    "plt.figure()\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"SER\")\n",
    "plt.semilogy(SNR_dB, SER_theory, label=\"Standard_16QAM_AWGN\")\n",
    "plt.semilogy(SNR_dB, SER, \"-*\", label=\"AE_GS_{}QAM_AGWN\".format(CONFIG_TRAIN[\"M\"]))\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transmitted symbols using the encoder\n",
    "num_mess = 6400  # number of messages to test\n",
    "messages = torch.randint(0, CONFIG_TRAIN[\"M\"], size=(num_mess,)).to(device)\n",
    "one_hot = F.one_hot(messages).float()\n",
    "tx = encoder(one_hot)\n",
    "tx = tx.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Define the coordinates for 16-QAM constellation points\n",
    "points_I = np.array([1, 1, 1, 1, -1, -1, -1, -1, 3, 3, 3, 3, -3, -3, -3, -3]) / np.sqrt(\n",
    "    10\n",
    ")\n",
    "points_Q = np.array([3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3]) / np.sqrt(\n",
    "    10\n",
    ")\n",
    "\n",
    "# Plot both constellations\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot transmitted symbols\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(tx[:, 0], tx[:, 1])\n",
    "plt.xlabel(\"I\")\n",
    "plt.ylabel(\"Q\")\n",
    "plt.title(\"Transmitted Constellation\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot standard 16-QAM constellation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(points_I, points_Q, c=\"r\")\n",
    "plt.xlabel(\"I\")\n",
    "plt.ylabel(\"Q\")\n",
    "plt.title(\"Standard 16-QAM Constellation\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
