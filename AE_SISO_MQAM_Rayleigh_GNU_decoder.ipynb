{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from torch import nn\n",
                "from torch.optim import Adam, SGD\n",
                "from tqdm import tqdm\n",
                "import os\n",
                "\n",
                "from utils import Encoder, Decoder, additive_white_gaussian_noise_channel, ser_mqam_awgn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG_TRAIN = {\n",
                "    \"M\": 16,  # Number of constellation points\n",
                "    \"flag_train_model\": True,  # Flag to control training\n",
                "    \"training_snr\": 12,  # Training SNR (dB)\n",
                "    \"best_encoder_path\": \"./model/ae_siso_awgn_16qam_best_encoder.pth\",  # Path to save the best encoder\n",
                "    \"best_decoder_path\": \"./model/ae_siso_rayleigh_16qam_best_decoder.pth\",  # Path to save the best decoder\n",
                "}\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder = Encoder([CONFIG_TRAIN[\"M\"], 10, 10, 2]).to(device)\n",
                "decoder = Decoder([2, 20, 20, CONFIG_TRAIN[\"M\"]]).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint = torch.load(CONFIG_TRAIN[\"best_encoder_path\"], map_location=device)\n",
                "encoder.load_state_dict(checkpoint)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_decoder(decoder, loss, optimizer):\n",
                "    \"\"\"\n",
                "    Save the model to a file.\n",
                "    - decoder: the decoder model\n",
                "    - loss: the loss history\n",
                "    - optimizer: the optimizer\n",
                "    - model_type: \"latest\"(default) or \"best\"\n",
                "    \"\"\"\n",
                "    torch.save(\n",
                "        {\n",
                "            \"Decoder\": decoder.state_dict(),\n",
                "            \"loss\": loss,\n",
                "            \"optimizer\": optimizer.state_dict(),\n",
                "        },\n",
                "        CONFIG_TRAIN[\"best_decoder_path\"],\n",
                "    )\n",
                "\n",
                "\n",
                "def early_stopping(loss_hist, patience=5):\n",
                "    \"\"\"\n",
                "    Check if the training should be stopped early.\n",
                "    - loss_hist: list of loss values\n",
                "    - patience: number of epochs to wait before stopping\n",
                "    \"\"\"\n",
                "\n",
                "    if len(loss_hist) > patience:\n",
                "        if all(\n",
                "            x > loss_hist[-1] for x in loss_hist[-(patience + 1) :]\n",
                "        ):  # if the last patience losses are decreasing\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "\n",
                "def train_decoder(decoder, optimizer, iterations, loss_hist, batch_size):\n",
                "    criterion = nn.NLLLoss()  # negative log likelihood loss\n",
                "    best_loss = float(\"inf\")  # Initialize the best loss to infinity\n",
                "    try:\n",
                "        for iterator in tqdm(\n",
                "            range(len(loss_hist), iterations), desc=\"training process\"\n",
                "        ):\n",
                "            start_index = int(iterator * batch_size)\n",
                "            end_index = int((iterator + 1) * batch_size - 1)\n",
                "            messages_batch = messages[start_index:end_index]\n",
                "            rx_batch = rx[iterator * batch_size : (iterator + 1) * batch_size - 1]\n",
                "            y_pred = decoder(rx_batch)\n",
                "            loss = criterion(y_pred, messages_batch)\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            loss_hist.append(loss.item())\n",
                "\n",
                "            if loss.item() < best_loss:\n",
                "                best_loss = loss.item()\n",
                "                save_decoder(decoder, loss_hist, optimizer)\n",
                "\n",
                "            if early_stopping(loss_hist):\n",
                "                print(\"Early stopping\")\n",
                "                break\n",
                "        print(\"Training complete\")\n",
                "\n",
                "    except KeyboardInterrupt:\n",
                "        save_decoder(decoder, loss_hist, optimizer)\n",
                "        print(\"Training interrupted\")\n",
                "\n",
                "    save_decoder(decoder, loss_hist, optimizer)\n",
                "    # Plot the loss\n",
                "    plt.semilogy(loss_hist)\n",
                "    plt.xlabel(\"Epoch\")\n",
                "    plt.ylabel(\"Loss\")\n",
                "    plt.title(\"Training Loss\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CONFIG_TRAIN[\"flag_train_model\"]:\n",
                "    # check if there is a checkpoint to resume training\n",
                "    if os.path.exists(CONFIG_TRAIN[\"best_decoder_path\"]):\n",
                "        checkpoint = torch.load(CONFIG_TRAIN[\"best_decoder_path\"], map_location=device)\n",
                "        # load the model, optimizer a loss history\n",
                "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
                "\n",
                "    parameters = list(decoder.parameters())\n",
                "    optimizer = Adam(parameters, lr=0.01)\n",
                "\n",
                "    if os.path.exists(CONFIG_TRAIN[\"best_decoder_path\"]):\n",
                "        # optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
                "        loss_hist = checkpoint[\"loss\"]\n",
                "        print(f\"Resuming training from iterator {len(loss_hist)}\")\n",
                "    else:\n",
                "        loss_hist = []\n",
                "        print(\"Training from scratch\")\n",
                "\n",
                "    iterations = int(1e3)  # Total number of epochs to train\n",
                "    batch_size = int(1e4)  # Number of messages to use for training (batch size)\n",
                "    if iterations > len(loss_hist):\n",
                "        num_messages = iterations * batch_size - len(loss_hist)\n",
                "        messages = torch.randint(0, 16, size=(num_messages + 8,), device=device)\n",
                "        one_hot = F.one_hot(messages, 16).float()\n",
                "        tx = encoder(one_hot)\n",
                "        # write tx to a binary file\n",
                "        tx = tx.detach().numpy()\n",
                "        with open(\"./file/tx.dat\", \"wb\") as f:\n",
                "            f.write(tx.tobytes())\n",
                "        # run channel.py to generate rx\n",
                "        os.system(\"python3 ./gnuradio/fading_awgn_model.py\")\n",
                "        # read rx from a binary file\n",
                "        with open(\"./file/rx.dat\", \"rb\") as f:\n",
                "            rx = np.frombuffer(f.read(), dtype=np.float32)\n",
                "        rx = torch.from_numpy(rx).to(device)\n",
                "        rx = rx.view(-1, 2)\n",
                "        messages = messages[3:-5]\n",
                "        train_decoder(decoder, optimizer, iterations, loss_hist, batch_size)\n",
                "    else:\n",
                "        print(\"epochs already completed\")\n",
                "else:\n",
                "    # check if there is a checkpoint to load the model\n",
                "    if os.path.exists(CONFIG_TRAIN[\"best_decoder_path\"]):\n",
                "        checkpoint = torch.load(CONFIG_TRAIN[\"best_decoder_path\"], map_location=device)\n",
                "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
                "        print(\"Model loaded. Training iterator: \", len(checkpoint[\"loss\"]))\n",
                "    else:\n",
                "        print(\n",
                "            \"Model not found, please set flag_train_model to True and train the model\"\n",
                "        )\n",
                "        exit(1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_messages = int(1e4)\n",
                "messages = torch.randint(0, 16, size=(num_messages + 8,), device=device)\n",
                "one_hot = F.one_hot(messages, 16).float()\n",
                "tx = encoder(one_hot)\n",
                "# write tx to a binary file\n",
                "tx = tx.detach().numpy()\n",
                "with open(\"./file/tx.dat\", \"wb\") as f:\n",
                "    f.write(tx.tobytes())\n",
                "# run channel.py to generate rx\n",
                "os.system(\"python3 ./gnuradio/fading_awgn_model.py\")\n",
                "# read rx from a binary file\n",
                "with open(\"./file/rx.dat\", \"rb\") as f:\n",
                "    rx = np.frombuffer(f.read(), dtype=np.float32)\n",
                "rx = torch.from_numpy(rx).to(device)\n",
                "rx = rx.view(-1, 2)\n",
                "messages = messages[3:-5]\n",
                "\n",
                "# load the best decoder\n",
                "checkpoint = torch.load(CONFIG_TRAIN[\"best_decoder_path\"], map_location=device)\n",
                "decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
                "\n",
                "# calculate the SER\n",
                "y_pred = decoder(rx)\n",
                "m_hat = torch.argmax(y_pred, -1)\n",
                "err = torch.sum(torch.not_equal(messages, m_hat)).to(\"cpu\").detach().numpy()\n",
                "SER = err / len(messages)\n",
                "print(\"SER: \", SER)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "torch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
