{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from utils import Encoder, Decoder, awgn, ser_mqam_awgn  # Import custom utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"M\": 16,  # Number of constellation points\n",
    "    \"flag_train_model\": True,  # Flag to control training\n",
    "    \"training_snr\": 20,  # Training SNR (dB)\n",
    "    \"checkpoint_file\": \"./model/ae_siso_awgn_16qam.pth\",\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder([CONFIG[\"M\"], 10, 10, 2]).to(device)\n",
    "decoder = Decoder([2, 20, 20, CONFIG[\"M\"]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, loss):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"Encoder\": encoder.state_dict(),\n",
    "            \"Decoder\": decoder.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        CONFIG[\"checkpoint_file\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(encoder, decoder, optimizer, num_epochs, loss_hist, device):\n",
    "    criterion = nn.NLLLoss()  # negative log likelihood loss\n",
    "    try:\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"training process\"):\n",
    "            messages = torch.randint(0, CONFIG[\"M\"], size=(64000,), device=device)\n",
    "            one_hot = F.one_hot(messages, CONFIG[\"M\"]).float()\n",
    "            tx = encoder(one_hot)\n",
    "            rx = awgn(tx, CONFIG[\"training_snr\"])\n",
    "            y_pred = decoder(rx)\n",
    "\n",
    "            loss = criterion(y_pred, messages)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist.append(loss.item())\n",
    "\n",
    "        save_model(encoder, decoder, loss_hist)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "        # Plot the loss\n",
    "        plt.semilogy(loss_hist)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        save_model(encoder, decoder, loss_hist)\n",
    "        print(\"Training interrupted\")\n",
    "\n",
    "\n",
    "if CONFIG[\"flag_train_model\"]:\n",
    "    parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = Adam(parameters, lr=0.0001)\n",
    "    num_epochs = int(1e3)\n",
    "    # check if there is a checkpoint to resume training\n",
    "    if os.path.exists(CONFIG[\"checkpoint_file\"]):\n",
    "        checkpoint = torch.load(CONFIG[\"checkpoint_file\"], map_location=device)\n",
    "        encoder.load_state_dict(checkpoint[\"Encoder\"])\n",
    "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
    "        loss_hist = checkpoint[\"loss\"]  # Use a different variable name\n",
    "        print(f\"Resuming training from epoch {len(loss_hist)}\")\n",
    "    else:\n",
    "        loss_hist = []  # Initialize the loss list\n",
    "        print(\"Training from scratch\")\n",
    "    train_model(encoder, decoder, optimizer, num_epochs, loss_hist, device)\n",
    "else:\n",
    "    # check if there is a checkpoint to load the model\n",
    "    if os.path.exists(CONFIG[\"checkpoint_file\"]):\n",
    "        checkpoint = torch.load(CONFIG[\"checkpoint_file\"], map_location=device)\n",
    "        encoder.load_state_dict(checkpoint[\"Encoder\"])\n",
    "        decoder.load_state_dict(checkpoint[\"Decoder\"])\n",
    "        print(\"Model loaded\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Model not found, please set flag_train_model to True and train the model\"\n",
    "        )\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_dB = np.arange(0, 22, 2)\n",
    "SER = np.array([])\n",
    "\n",
    "iteration = tqdm(range(len(SNR_dB)), desc=\"simulation process\")\n",
    "for iterator in iteration:\n",
    "    snr = SNR_dB[iterator]\n",
    "    num_mess = 6400  # number of messages to test\n",
    "    minErr = 1  # minimum number of errors\n",
    "    minSym = 1e6  # minimum number of symbols\n",
    "    totSym = 0  # total number of symbols\n",
    "    totErr = 0  # total number of errors\n",
    "    while totErr < minErr or totSym < minSym:\n",
    "        messages = torch.randint(0, CONFIG[\"M\"], size=(num_mess,)).to(device)\n",
    "        one_hot = F.one_hot(messages).float()\n",
    "        tx = encoder(one_hot)\n",
    "        rx = awgn(tx, snr)\n",
    "        rx_constant = (\n",
    "            rx.clone().detach().requires_grad_(False)\n",
    "        )  # no gradients in the channel model\n",
    "\n",
    "        y_pred = decoder(rx_constant)\n",
    "\n",
    "        m_hat = torch.argmax(y_pred, -1)\n",
    "\n",
    "        err = torch.sum(torch.not_equal(messages, m_hat)).to(\"cpu\").detach().numpy()\n",
    "\n",
    "        totErr += err\n",
    "        totSym += num_mess\n",
    "    SER = np.append(SER, totErr / totSym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SER_theory = ser_mqam_awgn(CONFIG[\"M\"], SNR_dB)\n",
    "# plot the SER-SNR curve\n",
    "plt.figure()\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"SER\")\n",
    "plt.semilogy(SNR_dB, SER_theory, label=\"Standard_16QAM_AWGN\")\n",
    "plt.semilogy(SNR_dB, SER, \"-*\", label=\"AE_GS_{}QAM_AGWN\".format(CONFIG[\"M\"]))\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transmitted symbols using the encoder\n",
    "num_mess = 6400  # number of messages to test\n",
    "messages = torch.randint(0, CONFIG[\"M\"], size=(num_mess,)).to(device)\n",
    "one_hot = F.one_hot(messages).float()\n",
    "tx = encoder(one_hot)\n",
    "tx = tx.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Define the coordinates for 16-QAM constellation points\n",
    "points_I = np.array([1, 1, 1, 1, -1, -1, -1, -1, 3, 3, 3, 3, -3, -3, -3, -3]) / np.sqrt(\n",
    "    10\n",
    ")\n",
    "points_Q = np.array([3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3]) / np.sqrt(\n",
    "    10\n",
    ")\n",
    "\n",
    "# Plot both constellations\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot transmitted symbols\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(tx[:, 0], tx[:, 1])\n",
    "plt.xlabel(\"I\")\n",
    "plt.ylabel(\"Q\")\n",
    "plt.title(\"Transmitted Constellation\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot standard 16-QAM constellation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(points_I, points_Q, c=\"r\")\n",
    "plt.xlabel(\"I\")\n",
    "plt.ylabel(\"Q\")\n",
    "plt.title(\"Standard 16-QAM Constellation\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
