{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7unTPLL7VEC",
        "outputId": "e4369ce7-071b-49ca-c5d6-e96fbed1aebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from scipy.special import erfc\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from torch.optim import SGD, Adam\n",
        "from tqdm import tqdm\n",
        "from utils import Encoder, Decoder, awgn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A3ra51HH71gr"
      },
      "outputs": [],
      "source": [
        "M = 16 # number of constellation points\n",
        "flag_train_model = False # True: train model, False: load pre-trained model\n",
        "Path = \"./models/ae_simo_rayleigh_16qam.pth\"\n",
        "encoder = Encoder([M, 64, 64, 64, 2]).to(device)\n",
        "decoder = Decoder([8, 512, 512, 512, M]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "UqoFhXYh8RUU",
        "outputId": "2e523a78-4d9d-4c46-d3f5-749c2491081d"
      },
      "outputs": [],
      "source": [
        "if flag_train_model:\n",
        "    train_snr = 15 # training SNR in dB\n",
        "    criterion = nn.NLLLoss()     # negative log likelihood loss\n",
        "    para = list(encoder.parameters()) + list(decoder.parameters())  # get all parameters\n",
        "    opt = Adam(para, lr=0.01)\n",
        "    loss = []  # store the loss value\n",
        "    totEpoch = int(1e4)  # total number of epochs\n",
        "    iteration = tqdm(range(totEpoch), desc=\"loss\")\n",
        "\n",
        "    for iterator in iteration:\n",
        "        iteration.set_description(\"epoch={:}\".format(iterator))\n",
        "        messages = torch.randint(0, M, size=(51200,), device=device)  # generate\n",
        "        one_hot = F.one_hot(messages, M).float()  # convert to one hot encoding shape=(16000, M)\n",
        "        tx =encoder(one_hot)\n",
        "\n",
        "        ch_real1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_real2 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag2 = torch.randn(tx.shape[0]).to(device)\n",
        "        csi = torch.stack((ch_real1, ch_imag1, ch_real2, ch_imag2), dim=1)\n",
        "\n",
        "        rx_real1 = torch.mul(ch_real1, tx[:, 0]) - torch.mul(ch_imag1, tx[:, 1])\n",
        "        rx_imag1 = torch.mul(ch_imag1, tx[:, 0]) + torch.mul(ch_real1, tx[:, 1])\n",
        "        rx_real2 = torch.mul(ch_real2, tx[:, 0]) - torch.mul(ch_imag2, tx[:, 1])\n",
        "        rx_imag2 = torch.mul(ch_imag2, tx[:, 0]) + torch.mul(ch_real2, tx[:, 1])\n",
        "\n",
        "        rx = torch.stack((rx_real1, rx_imag1, rx_real2, rx_imag2), dim=1)\n",
        "\n",
        "        rx = awgn(rx, train_snr)\n",
        "\n",
        "        rx = torch.cat((rx, csi), dim=1)\n",
        "\n",
        "        y_pred = decoder(rx)\n",
        "\n",
        "        cross_entropy = criterion(y_pred, messages)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        cross_entropy.backward()\n",
        "        opt.step()\n",
        "\n",
        "        loss.append(cross_entropy.item())\n",
        "\n",
        "\n",
        "    # plot the loss\n",
        "    plt.figure()\n",
        "    plt.plot(loss)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    # save the model\n",
        "    torch.save({\n",
        "            'Encoder': encoder.state_dict(),\n",
        "            'Decoder': decoder.state_dict(),\n",
        "            }, Path)\n",
        "    print(\"Model saved\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "bQHIF7F3y_eA",
        "outputId": "c8a4f9bc-127e-4371-993c-b9fefdc4ac9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SNR:  0 SER:  0.9321894645690918\n",
            "SNR:  1 SER:  0.9324771165847778\n",
            "SNR:  2 SER:  0.931568443775177\n",
            "SNR:  3 SER:  0.9315266609191895\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m rx \u001b[38;5;241m=\u001b[39m awgn(rx, snr)\n\u001b[1;32m     26\u001b[0m rx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((rx, csi), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_pred, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m err \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m!=\u001b[39m messages)\u001b[38;5;241m.\u001b[39msum()\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Codes/Auto-Encoder-in-MIMO-System/utils.py:40\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "SNR_dB = np.arange(0, 20, 1)\n",
        "\n",
        "# Monte Carlo simulation of the SER for the trained model\n",
        "SER = np.array([])\n",
        "for snr in SNR_dB:\n",
        "    num_mess = 6400  # number of messages to test\n",
        "    minErr = 1  # minimum number of errors\n",
        "    minSym = 1e6  # minimum number of symbols\n",
        "    totSym = 0  # total number of symbols\n",
        "    totErr = 0  # total number of errors\n",
        "    while totErr < minErr or totSym < minSym:\n",
        "        messages = torch.randint(0, M, size=(num_mess,)).to(device)\n",
        "        one_hot = F.one_hot(messages).float()\n",
        "        tx = encoder(one_hot)\n",
        "        ch_real1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_real2 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag2 = torch.randn(tx.shape[0]).to(device)\n",
        "        csi = torch.stack((ch_real1, ch_imag1, ch_real2, ch_imag2), dim=1)\n",
        "        rx_real1 = torch.mul(ch_real1, tx[:, 0]) - torch.mul(ch_imag1, tx[:, 1])\n",
        "        rx_imag1 = torch.mul(ch_imag1, tx[:, 0]) + torch.mul(ch_real1, tx[:, 1])\n",
        "        rx_real2 = torch.mul(ch_real2, tx[:, 0]) - torch.mul(ch_imag2, tx[:, 1])\n",
        "        rx_imag2 = torch.mul(ch_imag2, tx[:, 0]) + torch.mul(ch_real2, tx[:, 1])\n",
        "        rx = torch.stack((rx_real1, rx_imag1, rx_real2, rx_imag2), dim=1)\n",
        "        rx = awgn(rx, snr)\n",
        "        rx = torch.cat((rx, csi), dim=1)\n",
        "        y_pred = decoder(rx)\n",
        "        pred = torch.argmax(y_pred, 1)\n",
        "        err = (pred != messages).sum()\n",
        "        totErr += err.cpu()\n",
        "        totSym += num_mess\n",
        "    SER = np.append(SER, totErr / totSym)\n",
        "    print(\"SNR: \", snr, \"SER: \", SER[-1])\n",
        "\n",
        "# Monte Carlo simulation of the SER for the Maximum Ratio Combining (MRC) receiver\n",
        "SER_mrc = np.array([])\n",
        "for snr in SNR_dB:\n",
        "    num_mess = 6400  # number of messages to test\n",
        "    minErr = 1  # minimum number of errors\n",
        "    minSym = 1e6  # minimum number of symbols\n",
        "    totSym = 0  # total number of symbols\n",
        "    totErr = 0  # total number of errors\n",
        "    while totErr < minErr or totSym < minSym:\n",
        "        messages = torch.randint(0, M, size=(num_mess,)).to(device)\n",
        "        one_hot = F.one_hot(messages).float()\n",
        "        tx = encoder(one_hot)\n",
        "        ch_real1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag1 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_real2 = torch.randn(tx.shape[0]).to(device)\n",
        "        ch_imag2 = torch.randn(tx.shape[0]).to(device)\n",
        "        csi = torch.stack((ch_real1, ch_imag1, ch_real2, ch_imag2), dim=1)\n",
        "        rx_real1 = torch.mul(ch_real1, tx[:, 0]) - torch.mul(ch_imag1, tx[:, 1])\n",
        "        rx_imag1 = torch.mul(ch_imag1, tx[:, 0]) + torch.mul(ch_real1, tx[:, 1])\n",
        "        rx_real2 = torch.mul(ch_real2, tx[:, 0]) - torch.mul(ch_imag2, tx[:, 1])\n",
        "        rx_imag2 = torch.mul(ch_imag2, tx[:, 0]) + torch.mul(ch_real2, tx[:, 1])\n",
        "        rx = torch.stack((rx_real1, rx_imag1, rx_real2, rx_imag2), dim=1)\n",
        "        rx = awgn(rx, snr)\n",
        "        rx = torch.cat((rx, csi), dim=1)\n",
        "        rx = rx[:, 0:2] + rx[:, 2:4]\n",
        "        pred = torch.argmin((rx[:, 0:2] - M)**2, 1)\n",
        "        err = (pred != messages).sum()\n",
        "        totErr += err.cpu()\n",
        "        totSym += num_mess\n",
        "    SER_mrc = np.append(SER_mrc, totErr / totSym)\n",
        "    print(\"SNR: \", snr, \"SER MRC: \", SER_mrc[-1])\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(SNR_dB, SER, \"o-\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"SER\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "qmt22BOyzUKr",
        "outputId": "cddb2ec1-336d-4f64-aab8-06b248b8bba4"
      },
      "outputs": [],
      "source": [
        "# Generate transmitted symbols using the encoder\n",
        "num_mess = 6400  # number of messages to test\n",
        "messages = torch.randint(0, M, size=(num_mess,)).to(device)\n",
        "one_hot = F.one_hot(messages).float()\n",
        "tx = encoder(one_hot)\n",
        "tx = tx.to(\"cpu\").detach().numpy()\n",
        "\n",
        "# Define the coordinates for 16-QAM constellation points\n",
        "points_I = np.array([1, 1, 1, 1, -1, -1, -1, -1, 3, 3, 3, 3, -3, -3, -3, -3]) / np.sqrt(10)\n",
        "points_Q = np.array([3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3, 3, 1, -1, -3]) / np.sqrt(10)\n",
        "\n",
        "# Plot both constellations\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot transmitted symbols\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(tx[:, 0], tx[:, 1])\n",
        "plt.xlabel(\"I\")\n",
        "plt.ylabel(\"Q\")\n",
        "plt.title(\"Transmitted Constellation\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot standard 16-QAM constellation\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(points_I, points_Q, c=\"r\")\n",
        "plt.xlabel(\"I\")\n",
        "plt.ylabel(\"Q\")\n",
        "plt.title(\"Standard 16-QAM Constellation\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
